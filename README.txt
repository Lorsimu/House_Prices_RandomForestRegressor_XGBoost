# House Prices: Advanced Regression Techniques

## Descripci√≥n
Este proyecto es una soluci√≥n para la competici√≥n de Kaggle "House Prices: Advanced Regression Techniques". El objetivo es predecir el precio final de venta de viviendas residenciales en Ames, Iowa, utilizando 79 variables explicativas que describen casi todos los aspectos de las casas.

El enfoque principal se centra en un preprocesamiento de datos mediante **Pipelines** y la comparaci√≥n de rendimiento entre dos modelos: **Random Forest** vs **XGBoost**.

## üõ†Ô∏è Tecnolog√≠as y Librer√≠as
- **Python 3**
- **XGBoost:** Algoritmo principal seleccionado por su popularidad y eficacia.
- **Scikit-Learn:** - `Pipeline` y `ColumnTransformer` para un c√≥digo limpio y reproducible.
  - `RandomForestRegressor` como modelo base para la comparaci√≥n.
  - `OneHotEncoder` y `SimpleImputer` para el tratamiento de datos.
- **Pandas & NumPy:** Manipulaci√≥n y an√°lisis de datos.

## ‚öôÔ∏è Metodolog√≠a y Preprocesamiento

### 1. Tratamiento de Datos Faltantes
Se realiz√≥ un an√°lisis detallado de los valores nulos:
- **Variables Num√©ricas:** Imputaci√≥n utilizando la mediana.
- **Variables Categ√≥ricas:** Imputaci√≥n constante ("Doesn't apply") para caracter√≠sticas donde la ausencia de dato simplica ausencia de la caracter√≠stica (ej: si no se tiene piscina, no hay dato de su calidad), seguido de codificaci√≥n **One-Hot**.

### 2. Selecci√≥n de Modelos
Se entrenaron y compararon dos enfoques distintos utilizando validaci√≥n cruzada (k-fold para el modelo random forest) y m√©trica MAE (Mean Absolute Error):

* **Random Forest:** Se prob√≥ con 100 y 1000 estimadores.
* **XGBoost Regressor:** Se ajust√≥ con `early_stopping` para evitar sobreajuste.

## üìä Resultados
El modelo **XGBoost** super√≥ significativamente al Random Forest, demostrando ser m√°s efectivo para este dataset tabular complejo.

| Modelo | MAE (Error Medio Absoluto) |
| :--- | :--- |
| Random Forest (1000 trees) | ~17,912 |
| **XGBoost** | **~16,784** |

*El modelo final utilizado para la sumisi√≥n a Kaggle fue XGBoost.*

## üöÄ Instalaci√≥n y Uso

1. Clonar el repositorio.
2. Instalar las dependencias necesarias:
   ```bash
   pip install -r requirements.txt

## ‚úíÔ∏è Autor
**Lorenzo Ji** - *Proyecto personal con recursos proporcionado por Kaggle*
* Perfil de GitHub: https://github.com/Lorsimu
* Correo acad√©mico: lorenzo.ji@estudiante.uam.es